{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVARIANCE MATRIX (always squared and symmetric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These formulas are faster\n",
    "\n",
    "(if each column is a sample):\n",
    "\n",
    "$$ \\Sigma = \\frac{1}{n}(X-\\mu)({X}-\\mu)^\\top $$\n",
    "\n",
    "(if each row is a sample):\n",
    "\n",
    "$$ \\Sigma = \\frac{1}{n}(X-\\mu)^\\top({X}-\\mu) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but, alternatively, we could also do:\n",
    "\n",
    "$$ var(x) = \\frac {1}{n} \\cdot \\sum_{i=1}^n({x_i} - \\mu_x)^2 $$\n",
    "\n",
    "$$ var(y) = \\frac {1}{n} \\cdot \\sum_{i=1}^n({y_i} - \\mu_y)^2 $$\n",
    "\n",
    "$$ cov(x, y) = cov(y, x) = \\frac {1}{n} \\cdot \\sum_{i=1}^n({x_i} - \\mu_x)({y_i} - \\mu_y) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "Consider the following \"design\" matrix $X$, in which **`each column`** is a sample:\n",
    "\n",
    "$$\n",
    "X = \\begin{matrix}\n",
    "\\begin{pmatrix}\n",
    "1 & -5 & 10 & 2 & -3 \\\\\n",
    "2 & 10 & -5 & -1 & 4\n",
    "\\end{pmatrix}\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "This matrix is a D $\\times$ N matrix, where D is the number of dimensions and N is the number of samples (in this case, D = 2 and N = 5).\n",
    "\n",
    "The covariance matrix must be a D $\\times$ D matrix, so, in this case will be a 2 $\\times$ 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECTION OF A VECTOR ONTO A SUBSPACE \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below formula is correct to compute the projection of a vector $\\vec{x}$ onto a subspace $u$ generated by a unit vector $\\vec{u}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{P}_{u}{x} = ({x}^T{u})\\vec{u} $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ ({x}^T{u}) $ indicates the length, and\n",
    "- $ \\vec{u} $ indicates the direction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saying that we can either maximize the length from the origin of the projected point onto the subspace or minimize the reconstruction error is the same thing.\n",
    "\n",
    "But why \"maximize\" ? Because we want to keep the variance high, in order to don't loose information.\n",
    "\n",
    "We will use the concept of \"maximize the length from the origin of the project point onto the subspace\", and in order to achieve this:\n",
    "\n",
    "$$ \\arg\\max_{\\mathbf{u}} \\frac{1}{N}\\sum_i^N \\left\\|   \\mathbb{P}_{\\mathbf{u}}\\mathbf{x}_i \\right\\|_2^2 $$\n",
    "\n",
    "- The **size** of the projection is measured with the $\\ell_2^2$ norm:\n",
    "\n",
    "$$ || \\mathbb{P}_{\\mathbf{u}}\\mathbf{x}||_2^2 = ||X||\\cdot|cos\\theta|$$\n",
    "\n",
    "since $ ||u||_2 = 1 $ and $ \\theta $ is the angle between $X$ and $u$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CENTERING A CLOUD OF POINTS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To center a cloud of points, we need to subtract the empirical mean from each point.\n",
    "\n",
    "NOTE: the empirical mean that we compute by considering the design matrix $X$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, would be something like: \n",
    "\n",
    "```python\n",
    "X-X.mean(axis=0)\n",
    "```\n",
    "\n",
    "where \"axis=0\" is the x-axis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROTATION MATRIX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the rotation matrix $R$:\n",
    "\n",
    "Given the covariance matrix $\\sum$, we do eigendecomposition:\n",
    "\n",
    "1. Compute the eigenvectors & eigenvalues of $\\sum$;\n",
    "2. Arrange the eigenvectors in descending order into a matrix $V$. The order is given by the associated eigenvalues.\n",
    "3. Do the transpose of $V$ in order to obtain the inverse of $V$.\n",
    "\n",
    "$V^T$ = $V^{-1}$\n",
    "\n",
    "4. The rotation matrix $R$ can be obtained as $R = V^{-1} \\cdot V^\\top$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that, the rotation matrix $R$ is orthogonal, so $R^\\top = R^{-1}$ and also that $R^\\top \\cdot R = I$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that, after a rotation, the covariance matrix is diagonal, which means the covariance matrix is decorrelated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPHERING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sphering means that the data is centered, rotated and then scaled on the x-axis by the quare root of the eigenvalues. More formally, this is the formula:\n",
    "\n",
    "$$ X_{sphr} = \\Sigma^{-1/2} \\cdot U^\\top \\cdot (X - \\mu)^\\top $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ \\Sigma $ is the covariance matrix (diagonal matrix of eigenvalues);\n",
    "- $ U $ is the matrix of eigenvectors (principal components). Recall that the eigenvectors are the directions of the principal components and that can be obtained by doing the eigendecomposition of the centered covariance matrix;\n",
    "- $ \\bar{X} $ is the centered data matrix. We obtain it by subtracting the empirical mean from each point.\n",
    "\n",
    "Recall that, after a rotation, the covariance matrix is diagonal (identity), which means the covariance matrix is decorrelated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMING A UNIT CIRCLE INTO AN ELLIPSOID"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the area of the unit circle (radius 1) is $\\pi$, instead of $\\pi r^2$.\n",
    "\n",
    "Let $ X = \\begin{matrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\end{matrix} $ and DET($X$) = 3.\n",
    "\n",
    "If we transform the vector subspace in which the unit circle is contained (stretching in this way also the unit circle), the area of the ellipsoid will be DET($X$) $\\cdot \\pi$.\n",
    "\n",
    "This concept works fine in an N-dimensional space too if the given matrix, in this case $X$, is a square matrix.\n",
    "\n",
    "If the Determinant equals zero, the matrix is singular (not invertible) and the transformation is not possible, hence the unit circle gets squished into a line."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3DMM (3D Morphable Model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this kind of stuff, we need to find the Principal Components of the data (usually given as a covariance matrix $X$).\n",
    "\n",
    "The Principal Components are the directions of the maximum variance of the data and correspond to the eigenvectors of the centered covariance matrix.\n",
    "\n",
    "Recall that the Principal Components are order by the eigenvalues in descending order.\n",
    "\n",
    "The eigenvector associated to the highest eigenvalue is the direction of the maximum variance of the data (PC1) and, if modified by a scalar, it will change the variance of the data in a significant way.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ S' = \\hat{S} + \\alpha \\cdot U_i $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ \\hat{S} $ is the average shape;\n",
    "- $ \\alpha_i $ is the shape coefficient;\n",
    "- $ U_i $ is the shape basis (the eigenvector associated to the largest eigenvalue)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-MEANS STEPS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Initialization:** `RANDOM` sample the $K$ centroids $\\{ {\\mu}_1, \\ldots, {\\mu}_k\\}$. A trick is using available points (just choose a few of them).\n",
    "\n",
    "- This random initialization can be far from the optimal solution.\n",
    "\n",
    "2. **Assignment:** assign each point to the closest centroid. \n",
    "\n",
    "- `VARIANCE MINIMIZER OBJECTIVE FUNCTION` $\\longrightarrow$ min $ \\mu, x =  \\sum_{i}^{N} || {x}_i -{\\mu}_k ||_2^2 $.\n",
    "\n",
    "- At the end of this step we should have a pair of aligned set $\\{ {x}_1, \\ldots, {x_n}\\}$ and $\\{ {y}_1, \\ldots, {y_n}\\}$.\n",
    "\n",
    "3. **Update:** \n",
    "\n",
    "- Now, it is the **inverse** of before. Now we consider the \"centroids\".\n",
    "- For every centroid $k=[1,\\ldots,K]$ **we center it**. How ? We compute the mean of all the points that are assigned to that centroid.\n",
    "\n",
    "- At the end of this step we should have a pair of aligned set $\\{ {x}_1, \\ldots, {x_n}\\}$ and $\\{ {y}_1, \\ldots, {y_n}\\}$ and **updated centroids** $\\{ {\\mu}_1, \\ldots, {\\mu}_k\\}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INVERSE TRANSFORM SAMPLING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse transform sampling is a probabilistic method used in machine learning to generate random samples from a given probability distribution. The method relies on the cumulative distribution function (CDF) of the distribution to generate the samples.\n",
    "\n",
    "To use inverse transform sampling, we need to first compute the CDF of the probability distribution we want to sample from. The CDF gives the probability that a random variable takes a value less than or equal to a given value. We can use this information to generate random samples from the distribution.\n",
    "\n",
    "The steps to use inverse transform sampling are as follows:\n",
    "\n",
    "1. Choose a probability distribution that you want to sample from. This could be any probability distribution, such as a normal distribution, uniform distribution, or any other distribution.\n",
    "\n",
    "2. Compute the cumulative distribution function (CDF) of the chosen probability distribution. The CDF gives the probability that a random variable takes a value less than or equal to a given value. It is defined as the integral of the probability density function (PDF) from negative infinity to the given value. The CDF is a monotonically increasing function that ranges from 0 to 1.\n",
    "\n",
    "3. Generate a uniform random variable U between 0 and 1. This can be done using any method for generating random numbers in the range [0, 1], such as using a pseudorandom number generator.\n",
    "\n",
    "4. Use the inverse of the CDF to compute the corresponding value of the random variable X that corresponds to the probability U. This step is the core of inverse transform sampling. To do this, we take the inverse of the CDF, which gives us a function that maps probabilities to corresponding values of X. We then apply this inverse function to U to get a value of X that corresponds to the probability U.\n",
    "\n",
    "5. Repeat steps 3 and 4 as many times as necessary to generate the desired number of random samples. Each time step 4 is performed, we generate a new value of X that corresponds to a different probability U. By repeating this process many times, we can generate a large number of random samples from the desired probability distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-MEANS ++"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In K-Means++, the initial centroids are chosen in a way that maximizes the chances of finding good quality clusters. The algorithm works as follows:\n",
    "\n",
    "Choose the first centroid randomly from the data points.\n",
    "\n",
    "For each data point, compute the distance to the nearest centroid that has already been chosen.\n",
    "\n",
    "Choose the next centroid from the data points, with a probability proportional to the squared distance to the nearest centroid.\n",
    "\n",
    "Repeat steps 2 and 3 until K centroids have been chosen.\n",
    "\n",
    "By selecting the initial centroids in this way, K-Means++ ensures that the initial centroids are well spread out and that they are likely to represent different clusters. This helps to avoid the problem of K-Means getting stuck in local optima or converging to suboptimal clusterings.\n",
    "\n",
    "After the initialization step, K-Means++ proceeds in the same way as K-Means. It iteratively assigns each data point to the nearest centroid, and then updates the centroids based on the mean of the assigned data points. The algorithm repeats until convergence or a maximum number of iterations is reached."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose ${\\mu}_1$ arbitrarily between data points.\n",
    "2. For $k=[2,\\ldots,K]$:\n",
    "    - **Inverse Transform Sampling** w.r.t. to distances between centroids\n",
    "    - $Pr[{\\mu}_k={x}_m] \\propto \\min_{k<k^{\\prime}} \\left| \\left| {x}_m - {\\mu}_k^{\\prime}\\right|\\right|_2^2 \\qquad$\n",
    "3. Repeat Lloydâ€™s method until convergence :\n",
    "    - **Assignment step:** $\\forall i\\in[1,N] \\quad y_i =  \\arg\\min_k || {x}_i -{\\mu}_k ||_2^2 $\n",
    "    - **Update step:** $\\forall k\\in[1,K] \\quad  {\\mu}_k \\leftarrow \\frac{\\sum_i \\delta\\{y_i=k\\}{x}_i}{\\sum_i \\delta\\{y_i=k\\}}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM (Gaussian Mixture Model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modes are given by the eigenvalues of the covariance matrix. The number of modes is given by the number of non-zero eigenvalues.\n",
    "\n",
    "`The n. of modes can be easily deduced from the text of the problem`.\n",
    "\n",
    "Example: if the problem says that the data is generated by a mixture of 2 Gaussians, then the number of modes is 2.\n",
    "\n",
    "Usually, the modes are the cardinality of Z --> $|Z|$ or the cardinality of the responsability vector --> $|\\gamma|$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBABILITY DENSITY FUNCTION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability density function is a function that describes the relative likelihood for a given random variable to take on a given value. It is a function of the random variable and is often denoted by $f(x)$ or $p(x)$.\n",
    "\n",
    "It is a linear combination of Gaussian distributions, which is called a Gaussian mixture model. The Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters.\n",
    "\n",
    "The linear combination between coefficients and mixing coefficients indicates also a categorical distribution $\\pi$. If the modes are $K$, then the categorical distribution is a $K$-dimensional vector and we have that $\\sum_{k=1}^K \\pi_k = 1$, with $\\pi_k \\geq 0$.\n",
    "\n",
    "Each Gaussian distribution is characterized by a mean $\\mu_k$ and a covariance matrix $\\Sigma_k$. \n",
    "\n",
    "The probability density function is given by:\n",
    "\n",
    "$$ p(x) = \\sum_{k=1}^K \\pi_k \\cdot \\mathcal{N}(x | \\mu_k, \\Sigma_k) $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathcal{N}(x|\\mu_k, \\Sigma_k)$ is the probability density function of a Gaussian distribution with mean $\\mu_k$ and covariance matrix $\\Sigma_k$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESPONSABILITIES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The responsability vector $\\gamma_k$ indicates the probability that a point $x$ may have been generated by the $k$-th Gaussian distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\gamma = p(z == k | x) = \\frac{\\mathcal{N} \\cdot (x | \\mu_k, \\sigma_k)\\cdot \\pi_k}{\\sum_{k\\in |\\gamma|} \\cdot \\mathcal{N}(x | \\mu_k, \\sigma_k)\\cdot \\pi_k} $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $N(x; \\mu_k, \\sigma_k)$ is the probability density function of the $k$-th Gaussian distribution;\n",
    "- $\\pi_k$ is the prior probability of the $k$-th Gaussian distribution.\n",
    "\n",
    "$\\pi_k$ are also called mixing coefficients.\n",
    "\n",
    "$\\gamma$[0] = 0.8 means that there's 80% probability that the point $x$ was generated by the first Gaussian distribution."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
