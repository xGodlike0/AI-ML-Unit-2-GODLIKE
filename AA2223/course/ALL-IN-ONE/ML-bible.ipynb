{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # DATA CORRELATION\n",
    "\n",
    " Comprendere il concetto di correlazione tra dati è fondamentale in Machine Learning poiché la correlazione può influenzare direttamente la qualità dei modelli di Machine Learning e le decisioni basate su di essi.\n",
    "\n",
    "La correlazione tra dati si riferisce alla relazione tra due o più variabili in un dataset. Se due variabili sono correlate, significa che c'è una relazione tra di esse, e che il valore di una variabile può influenzare il valore dell'altra variabile.\n",
    "\n",
    "Quando si lavora con i dati di Machine Learning, la correlazione tra le variabili può influire sulla capacità del modello di fare previsioni accurate. Ad esempio, se due variabili sono fortemente correlate, può essere necessario considerarle entrambe quando si costruisce un modello. D'altra parte, se due variabili non sono correlate, può non essere necessario considerarle entrambe.\n",
    "\n",
    "Inoltre, la correlazione tra i dati può anche influire sulla scelta degli algoritmi di Machine Learning da utilizzare. Alcuni algoritmi funzionano meglio con dati altamente correlati, mentre altri funzionano meglio con dati poco correlati.\n",
    "\n",
    "In generale, comprendere la correlazione tra i dati è essenziale per garantire che i modelli di Machine Learning siano accurati e affidabili, e che le decisioni basate su di essi siano appropriate e ben informate.\n",
    "\n",
    " <img width=\"75%\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/1920px-Correlation_examples2.svg.png\" />\n",
    "\n",
    " <img src=\"https://wtmaths.com/P384_z1.png\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEASURING CORRELATION\n",
    "\n",
    "Esistono diversi modi per misurare la correlazione tra i dati, ma i due metodi più comuni sono il coefficiente di correlazione di Pearson e il coefficiente di correlazione di Spearman.\n",
    "\n",
    "Il coefficiente di correlazione di Pearson misura la relazione lineare tra due variabili continue. Il coefficiente può variare da -1 a 1, dove -1 rappresenta una forte correlazione negativa, 0 rappresenta l'assenza di correlazione e 1 rappresenta una forte correlazione positiva.\n",
    "\n",
    "Il coefficiente di correlazione di Spearman, d'altra parte, è utilizzato per misurare la relazione tra due variabili ordinali o continue non normali. Questo coefficiente utilizza invece il grado di classifica delle variabili in modo da trovare la relazione monotona tra due variabili. Il coefficiente di correlazione di Spearman può variare da -1 a 1, dove -1 rappresenta una forte correlazione negativa, 0 rappresenta l'assenza di correlazione e 1 rappresenta una forte correlazione positiva.\n",
    "\n",
    "Per entrambi i coefficienti, i valori vicini a 0 indicano una bassa correlazione, mentre i valori vicini a 1 o -1 indicano una forte correlazione.\n",
    "\n",
    "In generale, il coefficiente di correlazione di Pearson è utilizzato per variabili continue normalmente distribuite, mentre il coefficiente di correlazione di Spearman è utilizzato per variabili ordinali o continue non normali. Tuttavia, l'uso di un coefficiente di correlazione dipende dalle specifiche caratteristiche dei dati e delle variabili coinvolte.\n",
    "\n",
    "<mark style=\"background-color: yellow;\">Quando il coefficiente di correlazione di Pearson è pari a -1, indica una perfetta relazione lineare negativa tra due variabili. Ciò significa che all'aumentare del valore di una variabile, il valore dell'altra variabile diminuisce di una quantità proporzionale, risultando in una linea retta con una pendenza negativa quando le due variabili sono rappresentate in un diagramma di dispersione.\n",
    "\n",
    "In altre parole, quando il coefficiente di correlazione di Pearson è pari a -1, le due variabili sono perfettamente negative correlate, il che significa che si muovono in direzioni opposte in modo lineare. Ciò implica che se una variabile aumenta, l'altra variabile diminuirà sempre, e viceversa.\n",
    "\n",
    "Ad esempio, se consideriamo la relazione tra la temperatura in Celsius e la temperatura in Fahrenheit, sappiamo che hanno una perfetta correlazione negativa, perché all'aumentare di una scala di temperatura, l'altra scala di temperatura diminuisce in modo lineare. Pertanto, il coefficiente di correlazione di Pearson tra queste due variabili sarebbe -1.</mark>\n",
    "\n",
    "<img width='70%' src=\"https://upload.wikimedia.org/wikipedia/commons/3/34/Correlation_coefficient.png\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of coordinates matrix:\n",
    "\n",
    "| X \t| Y |\n",
    "| ----------- | ----------- |\n",
    "| 0.1 |\t45\n",
    "| 0.1 |\t65\n",
    "| 0.2 |\t28\n",
    "| 0.3 |\t76\n",
    "| 0.5 |\t55\n",
    "| 0.6 |\t48\n",
    "| 0.9 |\t64\n",
    "| 1.1 |\t41\n",
    "| 1.5 |\t30\n",
    "| 1.8 |\t52\n",
    "| 1.8 |\t75\n",
    "| 1.9 |\t35\n",
    "| 2.1 |\t42\n",
    "| 2.2 |\t65\n",
    "| 3.0 |\t30\n",
    "| 3.6 |\t71"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEARSON'S CORRELATION COEFFICIENT\n",
    "\n",
    "https://www.youtube.com/watch?v=xZ_z8KWkhXE&list=LL&index=6&t=1034s\n",
    "\n",
    "### Recall that:\n",
    "\n",
    "<mark style=\"background-color: yellow;\">La covarianza di due serie è una misura statistica che indica la relazione lineare tra le due serie. La covarianza è una misura della variabilità congiunta delle due serie: se le due serie variano insieme in modo simile, la covarianza sarà positiva; se invece variano in modo opposto, la covarianza sarà negativa; se non c'è alcuna relazione lineare tra le due serie, la covarianza sarà zero.</mark>\n",
    "\n",
    "Formalmente, la covarianza tra due serie X e Y è data dalla seguente formula:\n",
    "\n",
    "$$\\operatorname{cov}(X,Y) = \\frac{\\sum\\limits_{i=1}^{n} (X_i - \\bar{X}) (Y_i - \\bar{Y})}{n-1}$$\n",
    "\n",
    "dove $X_i$ e $Y_i$ sono i valori osservati delle due serie, $\\bar{X}$ e $\\bar{Y}$ sono le loro medie campionarie e $n$ è il numero di osservazioni.\n",
    "\n",
    "Se la covarianza è positiva, significa che le due serie sono correlate positivamente: cioè, quando il valore di una serie aumenta, anche il valore dell'altra serie tende ad aumentare. Se la covarianza è negativa, significa che le due serie sono correlate negativamente: cioè, quando il valore di una serie aumenta, il valore dell'altra serie tende a diminuire. Se la covarianza è zero, significa che le due serie non sono correlate linearmente.\n",
    "\n",
    "La covarianza è una misura utile per capire la relazione tra due serie, ma ha un limite: la covarianza non tiene conto della scala delle due serie. Ad esempio, se si misura il peso di una persona in chilogrammi e in libbre, la covarianza tra le due serie sarà influenzata dal fatto che un chilogrammo è circa 2,2 libbre. Per questo motivo, in molti casi si utilizza il coefficiente di correlazione di Pearson o di Spearman, che sono covarianza normalizzata rispettivamente dai valori di deviazione standard e di rank.\n",
    "\n",
    "La formula della covarianza utilizza la deviazione dalla media campionaria per calcolare quanto le due serie X e Y si discostano dalla loro media. La somma delle deviazioni è un'informazione importante, ma se si utilizza direttamente la somma delle deviazioni nella formula, il risultato ottenuto sarà influenzato dal numero di osservazioni presenti nella serie.\n",
    "\n",
    "Per evitare questo problema, nella formula della covarianza si divide la somma delle deviazioni per il numero di osservazioni meno uno, cioè $n-1$. Questa divisione normalizza la somma delle deviazioni, in modo che il risultato della covarianza dipenda solo dalla relazione tra le due serie, e non dal numero di osservazioni.\n",
    "\n",
    "`In pratica, dividere per n-1 invece di n rende la covarianza una stima più accurata della vera covarianza delle popolazioni da cui sono state estratte le due serie. Questo rende la covarianza un'indicatore più affidabile della relazione tra le due serie, soprattutto quando il numero di osservazioni è relativamente piccolo.`\n",
    "\n",
    "### Let's start introducing the Pearson's correlation coefficient:\n",
    "\n",
    "`The 1st way in which we can write it is the following:`\n",
    "\n",
    "$ \\rho_{X,Y}= \\frac{\\operatorname{cov}(X,Y)}{\\sigma_X \\sigma_Y}$\n",
    "\n",
    "where:\n",
    "* $ \\operatorname{cov} $ is the covariance of the two series that plot the coordinates of each point of a cloud of points\n",
    "* $ \\sigma_X $ is the standard deviation of $X $\n",
    "* $ \\sigma_Y $ is the standard deviation of  $ Y $\n",
    "\n",
    "The formula for $\\rho$ can be expressed in terms of mean and expectation.\n",
    "\n",
    "$$\\operatorname{cov}(X,Y) = \\mathbb{E}[(X-\\mu_X)(Y-\\mu_Y)]$$\n",
    "\n",
    "So Pearson correlation $\\rho$ can also be written as:\n",
    "\n",
    "$$\\rho_{X,Y}=\\frac{\\mathbb{E}[(X-\\mu_X)(Y-\\mu_Y)]}{\\sigma_X\\sigma_Y}$$\n",
    "\n",
    "\n",
    "\n",
    "- The correlation coefficient ranges between −1 and 1. \n",
    "- An absolute value of exactly $\\pm 1$ implies that a **linear equation describes the relationship between X and Y perfectly, with all data points lying on a line**. \n",
    "- The correlation **sign is determined by the regression slope**: a value of +1 implies that all data points lie on a line for which Y increases as X increases, and vice versa for −1.\n",
    "- `0` means that there is no linear dependency between variables\n",
    "\n",
    "$$\n",
    "r = \\frac{\\Sigma((X_i - \\bar{X})(Y_i - \\bar{Y}))}{\\sqrt{(\\Sigma((X_i - \\bar{X})^2 \\Sigma(Y_i - \\bar{Y})^2))}}\n",
    "$$\n",
    "\n",
    "- It takes maximum intensity when numerator is equal to denumerator. Otherwise Covariance is Always less than the product of the std. deviation\n",
    "- The **sign of the covariance** tells you if the data is **correlated** or **anticorrelated**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEARSON'S CORRELATION COEFFICIENT GEOMETRY:\n",
    "\n",
    "<img src=\"figs01/Geometry1.PNG\" style=\"width:30%\">\n",
    "<img src=\"figs01/Geometry2.PNG\" style=\"width:30%\">\n",
    "<img src=\"figs01/Geometry3.PNG\" style=\"width:30%\">\n",
    "<img src=\"figs01/Geometry4.PNG\" style=\"width:30%\">\n",
    "<img src=\"figs01/Geometry5.PNG\" style=\"width:30%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final note:  estimation $\\rightarrow$ Predictive power for future data\n",
    "\n",
    "<img src=\"figs01/Geometry6.PNG\" style=\"width:30%\">\n",
    "<img src=\"figs01/Geometry7.PNG\" style=\"width:30%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final note: more samples we have, the better we predict !\n",
    "\n",
    "<img src=\"figs01/Geometry8.PNG\" style=\"width:40%\">\n",
    "<img src=\"figs01/Geometry9.PNG\" style=\"width:40%\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDUCTIVE VS. DEDUCTIVE REASONING IN ML\n",
    "\n",
    "L'apprendimento induttivo e quello deduttivo sono due approcci fondamentali nell'apprendimento automatico (Machine Learning). Le principali differenze tra i due approcci sono le seguenti:\n",
    "\n",
    "`Approccio generale:` L'apprendimento induttivo è un approccio \"bottom-up\", che parte dai dati per estrarre una regola generale. L'apprendimento deduttivo è un approccio \"top-down\", che parte da una regola generale per applicarla a un insieme di dati specifici.\n",
    "\n",
    "`Sviluppo della conoscenza:` L'apprendimento induttivo sviluppa la conoscenza a partire dai dati osservati, identificando modelli o relazioni tra i dati stessi. L'apprendimento deduttivo si basa invece sulla conoscenza pregressa di regole o modelli che sono stati definiti in precedenza.\n",
    "\n",
    "`Flessibilità:` L'apprendimento induttivo è più flessibile rispetto all'apprendimento deduttivo, poiché è in grado di adattarsi a dati nuovi o inaspettati che possono essere incontrati durante il processo di apprendimento. L'apprendimento deduttivo, d'altra parte, è più rigido e richiede che i dati siano conformi alle regole o ai modelli predefiniti.\n",
    "\n",
    "`Complessità dei dati:` L'apprendimento induttivo è più adatto per modellare dati complessi e non lineari, mentre l'apprendimento deduttivo è più adatto per modellare dati strutturati e ben definiti.\n",
    "\n",
    "`Processo di apprendimento:` L'apprendimento induttivo richiede di esaminare una grande quantità di dati per identificare le relazioni e le tendenze tra di essi. L'apprendimento deduttivo richiede di definire le regole o i modelli prima di applicarli ai dati.\n",
    "\n",
    "In sintesi, l'apprendimento induttivo e quello deduttivo sono approcci complementari nell'apprendimento automatico, e la scelta dell'approccio più adatto dipende dalle caratteristiche del problema da risolvere e dei dati a disposizione.\n",
    "\n",
    "### INDUCTIVE LEARNING:\n",
    "\n",
    "<img src=\"figs01/19_supervised_example.PNG\" style=\"width:40%\">\n",
    "\n",
    "### TRANSDUCTIVE LEARNING:\n",
    "\n",
    "<img src=\"figs01/Transductive1.PNG\" style=\"width:20%\">\n",
    "<img src=\"figs01/Transductive2.PNG\" style=\"width:20%\">\n",
    "\n",
    "### LEARNING PARADIGMS\n",
    "\n",
    "### 1. Supervised learning (we have labels)\n",
    "\n",
    "Assume that there is a unknown and complex generator $\\mathcal{D}$ that provides output pairs $(\\mathbf{x},y)$.\n",
    "\n",
    "- We refer to this **unknown generator process as an unkown probability distribution** $\\mathcal{D}$ over input pairs  $(\\mathbf{x},y) \\in \\mathcal{X}\\times \\mathcal{Y}$.\n",
    "\n",
    "- **Example:** pairs of images and a label as in the case of bird/non-bird\n",
    "\n",
    "  - $\\mathbf{x}$ corresponds to the image; \n",
    "  - $y$ to the label.\n",
    "\n",
    "- Given paired $(\\mathbf{x},y)$, we learn to predict the label when given as input unseen data.\n",
    "\n",
    "  - **Classification: the output is a discrete value (category)**\n",
    "     - Binary classification (0/1)\n",
    "     - Multi-class classification (1...N)\n",
    "  - **Regression: the output is a continous value (real-valued output)**\n",
    "\n",
    "In pratice, in a real-world problem **no one has access to $\\mathcal{D}$ because problems are too complex.**\n",
    "\n",
    "> Try to write a computer program to generate all possible natural images that you can find in the word. Is it easy ?\n",
    "  \n",
    "Let's assume here that we have access to $\\mathcal{D}$ as a python function `get_prob_under_D(x,y)` that takes as input a pair `(x,y)` and returns the probability of the pair under $\\mathcal{D}$.\n",
    "\n",
    "If so, we can define the **Bayes optimal classifier** as the classifier that:\n",
    " - for any test input $\\mathbf{x}^{\\prime}$, simply returns the $y^{\\prime}$ that maximizes `get_prob_under_D(x,y)` \n",
    " - Or else, try all possible labels and return the label which yields maximum probability.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "h({x}^{\\prime}) = \\arg\\max_{y^{\\prime} \\in \\mathcal{Y} } \\mathcal{D}(x^{\\prime},y^{\\prime})\n",
    "\\end{equation}\n",
    "\n",
    "### 2. Unsupervised learning (we do NOT have labels)\n",
    "\n",
    "## TAKE AWAY\n",
    "The take-home message is that if someone gave us access to the \"data distribution\", forming an **optimal classifier would be trivial.**\n",
    "\n",
    "## REAL WORLD\n",
    "Unfortunately, no one gives us the implementation of this distribution.\n",
    "- We need to figure out ways of **learning the mapping from x to y**, given **only access to a training set sampled from $\\mathcal{D}$**, rather than $\\mathcal{D}$ itself.\n",
    "\n",
    "## TRAINING SET\n",
    "\n",
    "\\begin{equation}\n",
    " \\underbrace{\\{\\mathbf{x}_i,y_i\\}_{i=1}^N}_{\\text{known}} \\sim \\underbrace{\\mathcal{D}}_{\\text{unknown}}\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "- $N$ is the number of training samples\n",
    "- the vector $\\mathbf{x}$ is the input data\n",
    "- $y$ is the associated (scalar) label\n",
    "\n",
    "## SUPERVISED LEARNING FINAL DEFINITION\n",
    "\n",
    "**Goal:** given a training set with labels, learn a function over a set of possibile functions (hypothesis over a hypothesis set) \n",
    "\n",
    "$$h \\in \\mathcal{H}\\text{ so that }h : \\mathbf{x} \\mapsto y$$\n",
    "\n",
    "**Output of the learning is $h(\\cdot)$** that can be used to do prediction at test-time.\n",
    "\n",
    "**Prediction:** classification (discrete-valued) vs regression (real-valued output).\n",
    "\n",
    "<img src=\"figs01/09_ml_approach.PNG\" style=\"width:20%\">\n",
    "\n",
    "<img src=\"figs01/19_supervised_example.png\" style=\"width:40%\">\n",
    "\n",
    "## UNSUPERVISED LEARNING FINAL DEFINITION\n",
    "\n",
    "\\begin{equation}\n",
    " \\underbrace{\\{\\mathbf{x}_i\\}_{i=1}^N}_{\\text{known yet no labels}} \\sim \\underbrace{\\mathcal{D}}_{\\text{unknown}}\n",
    "\\end{equation}\n",
    "\n",
    "- We do not have any labels paired with the data.\n",
    "- Create an internal representation of the input, **capturing regularities/structure** in data.\n",
    " - Examples: **form clusters**, **extract features**.\n",
    " - How do we know if a representation is good ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLUSTERING (unsupervised)\n",
    "\n",
    "Il clustering è un algoritmo di apprendimento non supervisionato utilizzato in \"Machine Learning\" per raggruppare insiemi di oggetti o dati in base alle loro caratteristiche o similitudini.\n",
    "\n",
    "In pratica, il clustering cerca di identificare gruppi o cluster di dati simili tra loro, basandosi su misure di distanza o similarità tra le loro caratteristiche. Ad esempio, se si hanno dati relativi a diversi clienti di un'azienda, il clustering potrebbe identificare gruppi di clienti con comportamenti di acquisto simili, o gruppi di clienti che hanno interessi simili.\n",
    "\n",
    "L'obiettivo principale del clustering è quello di suddividere un insieme di dati in gruppi omogenei o cluster, in modo da poterli analizzare e comprendere meglio. Inoltre, il clustering può essere utilizzato per identificare anomalie o casi particolari che si discostano dai gruppi principali, permettendo di individuare eventuali problemi o opportunità.\n",
    "\n",
    "Esistono diversi algoritmi di clustering, come il k-means, il DBSCAN, il clustering gerarchico e molti altri, ognuno dei quali utilizza un approccio diverso per raggruppare i dati in base alle loro caratteristiche. Il clustering è ampiamente utilizzato in applicazioni di \"Machine Learning\", come l'analisi dei dati, il riconoscimento di pattern, l'elaborazione di immagini e molti altri campi.\n",
    "\n",
    "### EXAMPLE:\n",
    "\n",
    "- Each column is the result of a clustering algorithm.\n",
    "- The input data lives in a 2D space.\n",
    "- Colors indicates the clustering results (which points should be considered together).\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png\" width=60% />\n",
    "\n",
    "<small>Image from [scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#clustering)</small>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING SET\n",
    "\n",
    "\\begin{equation}\n",
    " \\underbrace{\\{\\mathbf{x}_i,y_i\\}_{i=1}^N}_{\\text{known}} \\sim \\underbrace{\\mathcal{D}}_{\\text{unknown}}\n",
    "\\end{equation}\n",
    "\n",
    "### $\\mathbf{x}$ as a high-dimensional point in a vector space\n",
    "\n",
    "-  $\\mathbf{x} \\in \\mathbb{R}^D$ is a vector in D-dimensional real-space.\n",
    "-  All the vectors are identified by using another point that functions as **origin**, i.e. $\\mathbf{O}=(0, 0, 0)$.\n",
    "- Moreover, for this to work you need a **orthonormal basis vector** on which you can express your vector.\n",
    "-  $\\mathbf{\\vec{x}}$ is bold because means is a vectory; we drop $\\vec{}$ for clarity.\n",
    "-  $y$ is a scalar value (it is not bold).\n",
    "\n",
    "### RECALL THAT:\n",
    "\n",
    "In algebra lineare, una base ortonormale è una base di uno spazio vettoriale in cui i vettori sono tutti ortogonali tra loro e di norma unitaria. Ciò significa che la base è costituita da un insieme di vettori linearmente indipendenti, ciascuno con una lunghezza unitaria e tutti perpendicolari tra loro.\n",
    "\n",
    "In modo più formale, una base ortonormale per uno spazio vettoriale V di dimensione n è un insieme di n vettori v1, v2, ..., vn tali che:\n",
    "\n",
    "ogni vettore vi ha una norma unitaria, cioè ||vi||=1 per ogni i;\n",
    "ogni coppia di vettori vi e vj è ortogonale, cioè il loro prodotto scalare è nullo se i≠j, cioè vi∙vj = 0 se i ≠ j e vi∙vi=1 per ogni i.\n",
    "La definizione di base ortonormale è molto importante in algebra lineare perché tali basi semplificano notevolmente la manipolazione di vettori e matrici. In particolare, ogni vettore in uno spazio vettoriale può essere espresso come una combinazione lineare dei vettori della base ortonormale, e questo rende più facile l'analisi delle proprietà dei vettori e delle matrici.\n",
    "\n",
    "Inoltre, molte applicazioni dell'algebra lineare, come la decomposizione in valori singolari e la trasformata di Fourier, si basano su basi ortonormali, il che rende la nozione di base ortonormale una delle nozioni fondamentali dell'algebra lineare.\n",
    "\n",
    "### VECTORS ARE WRITTEN COLUMN-WISE\n",
    "\n",
    "$$\\mathbf{x} =  \n",
    "\\begin{bmatrix}\n",
    "x_0, \\\\\n",
    "x_1, \\\\\n",
    "\\ldots, \\\\\n",
    "x_D \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "### TO MAKE IT ROW-WISE JUST TRANSPOSE IT\n",
    "\n",
    "$$\\mathbf{x}^{T} = \\begin{bmatrix}\n",
    "x_0, & x_1, & \\ldots, x_D \\\\\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy (numerical Python)\n",
    "\n",
    "NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\n",
    "\n",
    "<img class=\"center\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/31/NumPy_logo_2020.svg/220px-NumPy_logo_2020.svg.png\" />\n",
    "\n",
    "### During the course, we will learn how to \"vectorize\" the code (i.e. avoiding for loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.5  3.2]\n",
      " [ 0.   1. ]\n",
      " [ 2.  -3. ]]\n",
      "Shape (3, 2)\n",
      "Number of dimension: 2\n",
      "Number of elements: 6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[2.5, 3.2], [0, 1], [2, -3]], dtype=np.float32)\n",
    "print(x)\n",
    "print(f\"Shape {x.shape}\")  # the shape is...\n",
    "print(f\"Number of dimension: {x.ndim}\")  # is a matrix (2 axis)\n",
    "print(f\"Number of elements: {x.size}\")  # with 6 elements\n",
    "\n",
    "v = np.array([2.5, 3.2])  # used later "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LET'S TRY TO PLOT A VECTOR\n",
    "\n",
    "$$\\mathbf{x}^{T} = \\begin{bmatrix} 2.5, 3.2 \\\\\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['Times'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Font family ['Times'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Font family ['Times'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "font = {'family': 'Times',\n",
    "        'weight': 'bold',\n",
    "        'size': 22}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(v[0], v[1], marker='x', color='red', lw=4, markersize=6)\n",
    "# all the X first, then all the Y\n",
    "plt.plot([0, v[0]], [0, v[1]], marker='x', color='red', lw=4, markersize=6)\n",
    "# Eyecandy\n",
    "plt.axis('equal')\n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.axhline(0, color='black')\n",
    "plt.axvline(0, color='black')\n",
    "plt.title(\"Vector\")\n",
    "plt.annotate('(0, 0)', xy=(0, 0), xytext=(.1, -.3))\n",
    "plt.annotate(f'({v[0]},{v[1]})', xy=(v[0], v[1]), xytext=(v[0], v[1]))\n",
    "plt.axis([-5, 5, -5, 5])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VECTOR TO VECTOR OPERATIONS\n",
    "\n",
    "## INNER PRODUCT (AKA DOT PRODUCT)\n",
    "\n",
    "\\begin{equation} \\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^D ~~~~~~~~~\\quad \n",
    "\\mathbf{x}^T \\mathbf{y} = \\left\\langle\\mathbf{x}, \\mathbf{y}\\right\\rangle = \\sum_i^D \\mathbf{x}_i \\cdot \\mathbf{y}_i\n",
    "\\end{equation}\n",
    "\n",
    "```\n",
    "x1 x2 x3 x4 \n",
    "           y1\n",
    "           y2   =    result (dot_product)\n",
    "           y3\n",
    "           y4\n",
    "\n",
    "\n",
    "dot_product = x1y1 + x2y2 + x3y3 + x4y4\n",
    "```\n",
    "\n",
    "- The result is a **scalar** (not a vector anymore).\n",
    "- Must be in the same dimension.\n",
    "- It is **commutative**.\n",
    "- The data is **paired**: just muliply elementwise and sum across axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "y = np.array([1, 0, 1])\n",
    "np.dot(x, y) == np.sum(x*y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INNER PRODUCT GEOMETRIC INTERPRETATION\n",
    "\n",
    "Il dot product, noto anche come prodotto scalare, è una operazione matematica che prende in input due vettori e restituisce un valore scalare. In altre parole, il dot product tra due vettori restituisce la somma del prodotto tra i componenti corrispondenti dei due vettori.\n",
    "\n",
    "`Geometria`: il dot product viene utilizzato per calcolare l'angolo tra due vettori e per proiettare un vettore su un altro.\n",
    "\n",
    "`Informatica`: il dot product viene utilizzato in molte applicazioni di machine learning e computer vision, dove viene utilizzato per calcolare la similarità tra vettori e per effettuare il calcolo del prodotto tra matrici.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/d2l-ai/d2l-en/master/img/vec-angle.svg\" width=\"25%\">\n",
    "\n",
    "$$\n",
    "\\mathbf{v}^{\\top}\\cdot\\mathbf{w} = \\|\\mathbf{v}\\|\\|\\mathbf{w}\\|\\cos(\\theta).\n",
    "$$\n",
    "\n",
    "p.s. V transposed might be an error.\n",
    "\n",
    "With some simple algebraic manipulation, we can rearrange terms to obtain\n",
    "\n",
    "$$\n",
    "\\theta = \\arccos\\left(\\frac{\\mathbf{v}\\cdot\\mathbf{w}}{\\|\\mathbf{v}\\|\\|\\mathbf{w}\\|}\\right).\n",
    "$$\n",
    "\n",
    "\n",
    "This is a nice result since nothing in the computation references two-dimensions.\n",
    "\n",
    "**Indeed, we can use this in three or three million dimensions without issue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle(v, w):\n",
    "    return np.arccos(v.dot(w) / (np.linalg.norm(v) * np.linalg.norm(w)))\n",
    "\n",
    "angle(np.array([0, 1, 2]), np.array([2, 3, 4])) # the result is in radians"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COSINE SIMILARITY\n",
    "\n",
    "La similarità del coseno, anche chiamata coseno di similarità o coseno tra due vettori, è una misura di similarità utilizzata in molte applicazioni di machine learning e analisi dei dati.\n",
    "\n",
    "Essa viene utilizzata per confrontare la similarità tra due vettori, calcolando il coseno dell'angolo tra di essi. Il valore risultante può variare tra -1 e 1, dove 1 indica che i due vettori sono identici, 0 indica che sono completamente diversi e -1 indica che i due vettori sono opposti.\n",
    "\n",
    "In pratica, il calcolo della similarità del coseno può essere utilizzato per confrontare due documenti o due insiemi di parole, dove ogni documento o insieme di parole è rappresentato come un vettore di frequenza delle parole. In questo caso, la similarità del coseno viene calcolata come il coseno dell'angolo tra i due vettori di frequenza delle parole.\n",
    "\n",
    "La similarità del coseno può anche essere utilizzata in altri contesti, ad esempio per confrontare i profili degli utenti in un sistema di raccomandazione o per confrontare le caratteristiche di due immagini in un sistema di ricerca di immagini.\n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\underbrace{\\frac{\\mathbf{v}\\cdot\\mathbf{w}}{\\|\\mathbf{v}\\|\\|\\mathbf{w}\\|}}_{\\text{cosine similarity}}.\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTER PRODUCT\n",
    "\n",
    "L'outer product, anche chiamato prodotto esterno o prodotto tensore, è una operazione matematica che prende in input due vettori e restituisce una matrice. In particolare, il prodotto esterno tra un vettore riga e un vettore colonna restituisce una matrice, in cui ogni elemento è il prodotto degli elementi corrispondenti dei due vettori.\n",
    "\n",
    "Il prodotto esterno è utile in molti contesti matematici, tra cui:\n",
    "\n",
    "1. Algebra lineare: il prodotto esterno viene utilizzato per definire il prodotto tra due matrici e per definire le trasformazioni lineari.\n",
    "2. Informatica: il prodotto esterno viene utilizzato in molte applicazioni di machine learning e analisi dei dati, dove viene utilizzato per calcolare il prodotto tra matrici e per creare nuove feature da vettori di input.\n",
    "\n",
    "Ad esempio, il prodotto esterno può essere utilizzato per creare una matrice di covarianza a partire da un insieme di vettori di input, dove ogni elemento della matrice di covarianza rappresenta la covarianza tra due vettori di input. Inoltre, il prodotto esterno può essere utilizzato per creare nuove feature in un modello di machine learning, dove ogni elemento della matrice prodotto esterno rappresenta una combinazione lineare di due feature di input.\n",
    "\n",
    "$$ \\mathbf{x} \\in \\mathbb{R}^D, \\mathbf{y} \\in \\mathbb{R}^P $$\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x} \\mathbf{y}^T \\neq \\mathbf{y} \\mathbf{x}^T\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "```\n",
    "           P           ____________________\n",
    "      y1 y2 y3 y4     | x1 * yi for each Y |\n",
    "   x1                 |                    |\n",
    "D  x2              =  |                    |\n",
    "   x3                 |___________________ |\n",
    "```\n",
    "\n",
    "\n",
    "- The result is a **matrix** (not a vector anymore).\n",
    "- Input can have different dimensions. The output is $D \\times P$ dimensional.\n",
    "- It is **NOT commutative**.\n",
    "- The data is not paired $\\rightarrow$ compute all combinations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATRIX TO VECTOR OPERATION\n",
    "\n",
    "$\\mathbf{A} \\in \\mathbb{R}^{m \\times n} ~~$ \n",
    "$\\mathbf{x} \\in \\mathbb{R}^{n \\times 1}$  \n",
    "\n",
    "$\\mathbf{A}\\mathbf{x} =\\mathbf{b}$\n",
    "\n",
    "$$\n",
    "\\mathbf{A} =  \n",
    "\\begin{bmatrix}\n",
    "a_{11} & \\ldots & a_{1n} \\\\\n",
    "\\ldots &      & \\ldots \\\\\n",
    "a_{m1}& \\ldots& a_{mn} \\\\\n",
    "\\end{bmatrix}~~\n",
    "\\mathbf{x} =  \n",
    "\\begin{bmatrix}\n",
    "x_{11} \\\\\n",
    "\\ldots  \\\\\n",
    "x_{n1} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Two interpretations:\n",
    "1. Take each row of A, that is $\\mathbf{a}_r^T$, then do inner product with $\\mathbf{x}$.\n",
    "$$\n",
    "\\mathbf{b} = \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}_1^T\\mathbf{x}   \\\\\n",
    "\\ldots  \\\\\n",
    "\\mathbf{a}_m^T\\mathbf{x}  \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "2. Take each value of $\\mathbf{x}$, scale each column of A; sum across cols (axis=1).\n",
    "\n",
    "Il prodotto scalare tra le righe di una matrice e un vettore viene spesso utilizzato in algebra lineare e nell'apprendimento automatico per diversi scopi.\n",
    "\n",
    "Uno dei principali usi del prodotto scalare tra le righe di una matrice e un vettore è nella moltiplicazione tra una matrice e un vettore. In particolare, data una matrice A di dimensione m x n e un vettore x di dimensione n x 1, il prodotto Ax può essere calcolato come la combinazione lineare delle righe di A pesate dai corrispondenti elementi di x. In altre parole, l'i-esimo elemento del prodotto Ax è dato dal prodotto scalare tra la i-esima riga di A e il vettore x. Questa operazione è molto comune nell'apprendimento automatico, ad esempio nel calcolo delle previsioni di un modello di regressione lineare.\n",
    "\n",
    "Inoltre, il prodotto scalare tra le righe di una matrice e un vettore può essere utilizzato anche per misurare la similarità tra una matrice di dati e un dato di riferimento (ad esempio un vettore che rappresenta una media o un centroide). In questo caso, si calcola il prodotto scalare tra ogni riga della matrice e il vettore di riferimento, ottenendo un vettore di dimensione m che rappresenta la somiglianza tra ciascuna riga della matrice e il dato di riferimento.\n",
    "\n",
    "Infine, il prodotto scalare tra le righe di una matrice e un vettore può essere utilizzato anche per calcolare la norma delle righe della matrice. In particolare, la norma di una riga di una matrice è data dalla radice quadrata del prodotto scalare tra la riga stessa e se stessa. Questo tipo di calcolo può essere utile per normalizzare le righe di una matrice e renderle comparabili tra loro."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MATRIX-MATRIX MULTIPLICATION\n",
    "\n",
    "La moltiplicazione tra matrici può essere interpretata come una composizione di trasformazioni lineari. In particolare, se A e B sono due matrici, il prodotto AB può essere visto come una trasformazione lineare in cui si applica prima la trasformazione rappresentata da B e poi quella rappresentata da A.\n",
    "\n",
    "La moltiplicazione tra matrici è una trasformazione geometrica che modifica la posizione e l'orientamento degli oggetti. Per esempio, la moltiplicazione tra una matrice di rotazione e una matrice di traslazione produce una matrice che rappresenta una combinazione di rotazione e traslazione nello spazio.\n",
    "\n",
    "We have two matrices \n",
    "$\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ \n",
    "and $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$:\n",
    "\n",
    "- Note number of **columns** in $\\mathbf{A}$ must match number of **rows** in $\\mathbf{B}$. \n",
    "\n",
    "$$\\mathbf{A}=\\begin{bmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{B}=\\begin{bmatrix}\n",
    " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
    "\\end{bmatrix}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "\n",
      "[[0.4940718  0.2043658  0.04440585 0.0606546  0.44807493]\n",
      " [0.54608854 0.95271422 0.48195503 0.84878742 0.67582423]\n",
      " [0.80394927 0.08145095 0.74510979 0.38373028 0.71118063]]\n",
      "\n",
      "B\n",
      "\n",
      "[[0.3579596  0.25910049]\n",
      " [0.83910504 0.58511377]\n",
      " [0.20677484 0.96595641]\n",
      " [0.56610838 0.2270201 ]\n",
      " [0.87976774 0.2279624 ]]\n",
      "\n",
      "C\n",
      "\n",
      "[[0.78606308 0.40639966]\n",
      " [2.16963515 1.51123989]\n",
      " [1.35310393 1.22494223]]\n"
     ]
    }
   ],
   "source": [
    "A = np.random.rand(3, 5)\n",
    "B = np.random.rand(5, 2)\n",
    "# 3x2 = 3x5 @ 5x2\n",
    "C = A @ B\n",
    "print('A', A, 'B', B, 'C', C, sep='\\n\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHY MATRICES ?\n",
    "\n",
    "### 1. Good to model linear transformations in space\n",
    "### 2. Good to model the data.\n",
    "### 3. Express variations in data (covariance matrix is a symmetric matrix).\n",
    "### 4. Direction where to move to minimize loss (Gradients, deep learning)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NORMS\n",
    "\n",
    "Some of the most useful operators in linear algebra are *norms*.\n",
    "Informally, the norm of a vector tells us how **big** it is. \n",
    "\n",
    "A norm is a function $\\| \\cdot \\|$ that maps a vector\n",
    "to a scalar and satisfies the following three properties:\n",
    "\n",
    "1. Given any vector $\\mathbf{x}$, if we scale (all elements of) the vector \n",
    "   by a scalar $\\alpha \\in \\mathbb{R}$, its norm scales accordingly:\n",
    "   $$\\|\\alpha \\mathbf{x}\\| = |\\alpha| \\|\\mathbf{x}\\|.$$\n",
    "2. For any vectors $\\mathbf{x}$ and $\\mathbf{y}$:\n",
    "   norms satisfy the triangle inequality:\n",
    "   $$\\|\\mathbf{x} + \\mathbf{y}\\| \\leq \\|\\mathbf{x}\\| + \\|\\mathbf{y}\\|.$$\n",
    "3. The norm of a vector is nonnegative and it only vanishes if the vector is zero:\n",
    "   $$\\|\\mathbf{x}\\| > 0 \\text{ for all } \\mathbf{x} \\neq 0.$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_1$ norm\n",
    "\n",
    "**The $\\ell_1$ norm** is also popular \n",
    "and the associated metric is called the `Manhattan distance`. \n",
    "By definition, the $\\ell_1$ norm sums \n",
    "the absolute values of a vector's elements:\n",
    "\n",
    "**$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$$**\n",
    "\n",
    "Compared to the $\\ell_2$ norm, it is less sensitive to outliers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_2$ norm\n",
    "Many functions are valid norms and different norms \n",
    "encode different notions of size. \n",
    "The `Euclidean norm` that we all learned in elementary school geometry when calculating the hypotenuse of right triangle is the square root of the sum of squares of a vector's elements.\n",
    "Formally, this is called **the $\\ell_2$ *norm*** and expressed as\n",
    "\n",
    "**$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}.$$**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La norma l1 e la norma l2 sono due tipi di \"norme\" comunemente utilizzate in algebra lineare e nell'apprendimento automatico per valutare la grandezza di un vettore.\n",
    "\n",
    "La norma l1, nota anche come norma di Manhattan o distanza di Manhattan, è una misura della distanza tra due punti in uno spazio n-dimensionale.\n",
    "\n",
    "La norma l2, nota anche come norma euclidea, è una misura della grandezza di un vettore e rappresenta la distanza euclidea del vettore dall'origine.\n",
    "\n",
    "Entrambe le norme hanno diverse proprietà utili in algebra lineare e nell'apprendimento automatico. Ad esempio, la norma l1 viene spesso utilizzata per selezionare le caratteristiche più importanti di un modello di apprendimento automatico, in quanto è meno sensibile agli outlier rispetto alla norma l2. La norma l2, invece, viene spesso utilizzata per calcolare la distanza tra due vettori o per normalizzare un vettore in modo che abbia lunghezza unitaria.\n",
    "\n",
    "Quindi in sostanza nel campo del Machine Learning, le norme L1 ed L2 sono spesso utilizzate per regolarizzare i modelli di apprendimento automatico, in modo da evitare il fenomeno dell'overfitting e migliorare la capacità di generalizzazione dei modelli."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_p$ norm\n",
    "\n",
    "Both the $\\ell_2$ and $\\ell_1$ norms are special cases\n",
    "of the more general $\\ell_p$ *norms*:\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPLANES\n",
    "\n",
    "Gli hyperplane sono oggetti geometrici che dividono lo spazio in due parti. In particolare, in uno spazio di dimensione n, un hyperplane è un sottospazio di dimensione n-1. In altre parole, un hyperplane è una superficie piatta di dimensione n-1 che divide lo spazio in due regioni.\n",
    "\n",
    "Gli hyperplane sono spesso utilizzati in machine learning per problemi di classificazione e clustering. Ad esempio, in un problema di classificazione binaria, dove l'obiettivo è separare due classi di punti in uno spazio n-dimensionale, un hyperplane può essere utilizzato per definire il confine di decisione tra le due classi.\n",
    "\n",
    "Il confine di decisione, definito dall'hyperplane, è tale che tutti i punti di una classe si trovano da una parte dell'hyperplane, mentre tutti i punti dell'altra classe si trovano dall'altra parte dell'hyperplane.\n",
    "\n",
    "In particolare, in un problema di classificazione, il modello di apprendimento automatico cerca di trovare l'hyperplane che separa le due classi in modo ottimale. Ciò significa che l'hyperplane deve essere posizionato in modo che massimizzi la distanza tra i punti delle due classi più vicine all'hyperplane, anche chiamati support vector. Questa tecnica è nota come Support Vector Machines (SVM).\n",
    "\n",
    "Gli hyperplane possono essere utilizzati anche per risolvere problemi di clustering, dove l'obiettivo è trovare i gruppi di punti simili in uno spazio n-dimensionale. In questo caso, gli hyperplane possono essere utilizzati per dividere lo spazio in regioni in cui i punti sono più simili tra loro.\n",
    "\n",
    "In sintesi, gli hyperplane sono utili strumenti matematici per definire confini di decisione e dividere lo spazio in regioni in cui i punti sono più simili tra loro. Possono essere utilizzati in molte applicazioni di machine learning, tra cui classificazione e clustering.\n",
    "\n",
    "$ \\mathbf{w}\\mathbf{x} + \\mathbf{b} = 0$ where $\\mathbf{w}$ is a vector normal to the hyperplane and $\\mathbf{b}$ is an offset.\n",
    "\n",
    "<img width='30%' src='https://d2l.ai/_images/space-division.svg' />\n",
    "\n",
    "<img width='30%' src='https://d2l.ai/_images/space-division-3d.svg' />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECTION\n",
    "\n",
    "Suppose that we have two vectors $\\mathbf{v}$ and a column vector $\\mathbf{w}=[2,1]^\\top$.  \n",
    "\n",
    "**We want to project $\\mathbf{v}$ onto $\\mathbf{w}$ or better project v onto the subspace (line in this case) $\\mathbf{w}$.**\n",
    "\n",
    "Recalling trigonometry, we see the formula $\\|\\mathbf{v}\\|\\cos(\\theta)$ is the length of the projection of the vector $\\mathbf{v}$ onto the direction of $\\mathbf{w}$.\n",
    "\n",
    "<img width='30%' src=\"https://raw.githubusercontent.com/d2l-ai/d2l-en/master/img/proj-vec.svg\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting a vector onto the subspace defined by $\\mathbf{w}$\n",
    "\n",
    "$$\\mathbb{P}_\\mathbf{w}(\\mathbf{v}) = \\frac{\\mathbf{w}\\mathbf{w}^T}{\\mathbf{w}^T\\mathbf{w}}\\mathbf{v} = \\left( \\frac{\\mathbf{w}}{||\\mathbf{w}||}\\right)\\left(\\frac{\\mathbf{w}}{||\\mathbf{w}||}\\right)^T\\mathbf{v}$$\n",
    "\n",
    "Defining a unit vector $\\mathbf{\\hat{w}}=\\frac{\\mathbf{w}}{||\\mathbf{w}||}$ we have:\n",
    "$$\\mathbb{P}_\\mathbf{w}(\\mathbf{v}) =  \\underbrace{\\mathbf{\\hat{w}}}_{\\text{direction}} \\left(\\underbrace{\\mathbf{\\hat{w}}^T \\mathbf{v}}_{\\text{length}}\\right)$$\n",
    "\n",
    "- The projection must be on unit vector $\\alpha\\cdot\\mathbf{\\hat{w}}$.\n",
    "- How long in this direction? $\\alpha=\\mathbf{\\hat{w}}^T \\mathbf{v}$ that gives the lenght of **v** onto **w**.\n",
    "- **$\\mathbf{w}$** can be also a matrix not a vector (matrix which columns are vectors).\n",
    "\n",
    "La proiezione di un vettore su un sottospazio non deve necessariamente essere su un vettore unitario, ma spesso è conveniente esprimerla in termini di un vettore unitario.\n",
    "\n",
    "Per capire il motivo, facciamo prima una breve rassegna di cosa sia una proiezione. Dato un vettore u e un sottospazio V, la proiezione di u su V è il punto più vicino in V a u. In altre parole, è il vettore che giace in V ed è più vicino a u.\n",
    "\n",
    "La proiezione di u su V può essere calcolata come il prodotto scalare di u con un vettore unitario che è ortogonale al complemento del sottospazio, indicato come V⊥. Il motivo per cui si utilizza un vettore unitario in questo caso è che semplifica il calcolo della proiezione e, inoltre, è conveniente perché ci consente di interpretare la proiezione come la componente di u che giace nella direzione del vettore unitario.\n",
    "\n",
    "Inoltre, l'uso di un vettore unitario ha il vantaggio di fornire una rappresentazione normalizzata della proiezione, che può essere utile in alcune applicazioni. Ad esempio, se siamo interessati a misurare la similarità tra due vettori, l'uso della proiezione su un vettore unitario ci consente di confrontare le magnitudini dei due vettori indipendentemente dalle loro lunghezze.\n",
    "\n",
    "In sintesi, la proiezione di un vettore su un sottospazio non deve necessariamente essere su un vettore unitario, ma l'uso di un vettore unitario è spesso conveniente perché semplifica il calcolo della proiezione e fornisce una rappresentazione normalizzata che può essere utile in varie applicazioni."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EIGENDECOMPOSITION (aka finding a diagonal matrix)\n",
    "\n",
    "Suppose that we have a matrix $A$ with the following entries:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "2 & 0 \\\\\n",
    "0 & -1\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "If we apply $A$ to any vector $\\mathbf{v} = [x, y]^\\top$, \n",
    "we obtain a vector $\\mathbf{A}\\mathbf{v} = [2x, -y]^\\top$.\n",
    "This has an intuitive interpretation:\n",
    "stretch the vector to be twice as wide in the $x$-direction,\n",
    "and then flip it in the $y$-direction.\n",
    "\n",
    "**However, there are *some* vectors for which something remains unchanged.**\n",
    "\n",
    "Namely $[1, 0]^\\top$ gets sent to $[2, 0]^\\top$\n",
    "and $[0, 1]^\\top$ gets sent to $[0, -1]^\\top$.\n",
    "\n",
    "These vectors are still in the same line,\n",
    "and the only modification is that the matrix stretches them\n",
    "by a factor of $2$ and $-1$ respectively.\n",
    "**We call such vectors *eigenvectors*\n",
    "and the factor they are stretched by *eigenvalues*.**\n",
    "\n",
    "In general, if we can find a number $\\lambda$ \n",
    "and a vector $\\mathbf{v}$ such that \n",
    "\n",
    "$$\n",
    "\\underbrace{\\mathbf{A}}_{\\text{known}}\\mathbf{v} = \\underbrace{\\lambda\\mathbf{v}}_{\\text{unknown}}\n",
    "$$\n",
    "\n",
    "We say that $\\mathbf{v}$ is an eigenvector for $A$ and $\\lambda$ is an eigenvalue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRINCIPAL COMPONENTS ANALYSIS (PCA)\n",
    "\n",
    "I concetti di \"componenti principali\" e \"autovettori\" sono strettamente correlati in statistica e analisi dei dati.\n",
    "\n",
    "In analisi delle componenti principali (PCA), si cerca di ridurre la dimensionalità dei dati mantenendo la maggior parte possibile della loro varianza. Ciò viene fatto trasformando gli attributi originali in un nuovo insieme di variabili non correlate chiamate componenti principali. Queste componenti sono ordinate in modo tale che la prima componente spieghi la maggior parte possibile della varianza dei dati, la seconda spieghi il resto della varianza e così via.\n",
    "\n",
    "Le componenti principali sono calcolate come combinazione lineare degli attributi originali. Gli autovettori sono utilizzati per calcolare queste combinazioni lineari. Gli autovettori sono i vettori che soddisfano la seguente equazione: Ax = λx, dove A è una matrice quadrata, x è un vettore e λ è un numero. In altre parole, gli autovettori sono i vettori che quando moltiplicati per A danno come risultato un multiplo di se stessi.\n",
    "\n",
    "In PCA, la matrice di covarianza dei dati viene utilizzata come matrice A nella formula degli autovettori. Gli autovettori della matrice di covarianza rappresentano le direzioni principali di variazione dei dati e possono quindi essere utilizzati come base per calcolare le componenti principali. La prima componente principale corrisponde all'autovettore associato all'autovalore più grande, la seconda componente principale corrisponde all'autovettore associato al secondo autovalore più grande e così via.\n",
    "\n",
    "In sintesi, gli autovettori sono utilizzati in PCA per calcolare le combinazioni lineari delle variabili originali che massimizzano la varianza dei dati, dando origine alle componenti principali."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
